# LLM 서빙 벤치마크 메트릭 가이드

## 1. 개요

이 문서는 LLM 서빙 프레임워크(vLLM, SGLang, Ollama) 벤치마크 결과를 처음 접하는 분들을 위한 메트릭 해설서이다. A100 80GB GPU 환경에서 20B 파라미터 모델(`openai/gpt-oss-20b`)을 기준으로, 각 지표가 무엇을 의미하는지, 어떤 값이 좋은 것인지, 프레임워크 간 비교 시 주의할 점은 무엇인지를 정리한다.

벤치마크 숫자만 보면 "높으면 좋은 건가? 낮으면 좋은 건가?"부터 헷갈릴 수 있다. 이 가이드를 읽고 나면 결과 테이블의 숫자가 실제로 어떤 체감 성능을 의미하는지 감을 잡을 수 있을 것이다.

---

## 2. 핵심 메트릭 설명

### 2.1 TTFT (Time to First Token)

| 항목 | 내용 |
|------|------|
| **정의** | 요청을 보낸 시점부터 첫 번째 토큰이 생성되어 돌아오기까지 걸리는 시간. 스트리밍 응답에서 사용자가 "첫 글자"를 보기까지의 대기 시간이다. |
| **단위** | 밀리초(ms) |
| **낮을수록 좋다** | 값이 작을수록 사용자가 빠르게 응답을 받기 시작한다. |

**비유**: 식당에 주문을 넣고 첫 번째 요리가 나오기까지 기다리는 시간이다. 메인 요리 전체가 완성될 때까지 기다리는 게 아니라, 첫 접시(예: 에피타이저)가 테이블에 놓이는 순간까지의 시간이다. 첫 접시가 빨리 나오면 "아, 여기 빠르네"라고 느끼는 것과 같다.

**A100 + 20B 모델 참고 기준값**:

| 수준 | TTFT | 설명 |
|------|------|------|
| 좋음 | < 100ms | 사용자가 거의 지연을 느끼지 못함 |
| 보통 | 100 ~ 500ms | 체감할 수 있지만 허용 가능한 수준 |
| 나쁨 | > 1000ms | 사용자가 명확한 대기 시간을 느낌 |

> **참고**: TTFT는 입력 프롬프트 길이에 크게 영향을 받는다. 프롬프트가 길수록(prefill 연산이 많아지므로) TTFT가 늘어난다. 동시 요청이 많아도 TTFT가 증가한다.

---

### 2.2 Token Throughput (토큰 처리량)

| 항목 | 내용 |
|------|------|
| **정의** | 초당 생성되는 토큰 수. 단일 요청 기준(per-request)과 서버 전체 기준(aggregate)으로 나뉜다. |
| **단위** | tokens/sec (tok/s) |
| **높을수록 좋다** | 값이 클수록 같은 시간에 더 많은 텍스트를 생성한다. |

**비유**: 타자 속도(WPM, Words Per Minute)와 같다. 개인 타자 속도가 단일 요청 throughput이고, 사무실 전체 직원이 동시에 치는 총 타자량이 aggregate throughput이다.

**A100 + 20B 모델 참고 기준값**:

| 수준 | 단일 요청 | 배치/동시 처리 (aggregate) | 설명 |
|------|-----------|---------------------------|------|
| 좋음 | > 50 tok/s | > 1000 tok/s | 쾌적한 실시간 응답 |
| 보통 | 30 ~ 50 tok/s | 500 ~ 1000 tok/s | 일반적인 수준 |
| 나쁨 | < 20 tok/s | < 500 tok/s | 체감상 느림 |

> **참고**: 단일 요청 throughput은 사용자 한 명의 체감 속도를 나타내고, aggregate throughput은 서버의 전체 처리 능력을 나타낸다. 동시 요청이 많을수록 aggregate throughput은 올라가지만, 개별 사용자의 per-request throughput은 떨어질 수 있다.

---

### 2.3 Request Throughput (요청 처리량)

| 항목 | 내용 |
|------|------|
| **정의** | 서버가 초당 완료하는 요청(request) 수. 하나의 요청은 프롬프트 입력부터 전체 응답 생성 완료까지를 의미한다. |
| **단위** | requests/sec (req/s) |
| **높을수록 좋다** | 값이 클수록 동시에 더 많은 사용자를 처리할 수 있다. |

**비유**: 식당에서 주방이 시간당 처리하는 주문 건수이다. 한 시간에 주문 50건을 완료하는 주방과 20건을 완료하는 주방이 있다면, 50건 주방이 더 많은 손님을 받을 수 있다.

**A100 + 20B 모델 참고 기준값**:

| 수준 | 값 | 설명 |
|------|-----|------|
| 좋음 | > 5 req/s | 다수 사용자 동시 서빙 가능 |
| 보통 | 1 ~ 5 req/s | 소규모 서비스에 적합 |
| 나쁨 | < 1 req/s | 단일 사용자 수준 |

> **참고**: Request throughput은 생성하는 토큰 수(output length)에 크게 좌우된다. 짧은 응답이면 req/s가 높고, 긴 응답이면 낮아진다. 따라서 반드시 동일한 output length 조건에서 비교해야 한다.

---

### 2.4 Total Latency (총 지연 시간)

| 항목 | 내용 |
|------|------|
| **정의** | 요청을 보낸 시점부터 마지막 토큰이 생성될 때까지의 전체 소요 시간. TTFT + 전체 토큰 생성 시간을 포함한다. |
| **단위** | 밀리초(ms) 또는 초(s) |
| **낮을수록 좋다** | 값이 작을수록 전체 응답이 빠르게 완료된다. |

**비유**: 식당에 들어가서 모든 코스 요리를 다 먹고 나올 때까지의 시간이다. 첫 번째 접시(TTFT)가 빨리 나와도 디저트까지 2시간 걸리면, 총 소요 시간은 2시간이다.

**Total Latency는 다음과 같이 분해할 수 있다**:

```
Total Latency = TTFT + (생성 토큰 수 / Token Throughput)
```

예를 들어 TTFT가 50ms이고 200 토큰을 50 tok/s로 생성한다면:
- Total Latency = 50ms + (200 / 50) * 1000ms = 50ms + 4000ms = 4050ms (약 4초)

---

## 3. 백분위수 (Percentile) 이해하기

벤치마크 결과에서 p50, p95, p99 같은 표기를 자주 볼 수 있다. 이것은 **백분위수(percentile)**로, 전체 요청 중 해당 비율의 요청이 이 값 이하로 완료되었다는 뜻이다.

### 시험 점수로 비유하기

반 학생 100명이 시험을 봤다고 하자.

| 백분위수 | 의미 | 시험 비유 |
|---------|------|----------|
| **p50 (중앙값)** | 전체 요청의 50%가 이 값 이하 | 100명 중 50등의 점수. "평균적인 학생"의 성적 |
| **p95** | 전체 요청의 95%가 이 값 이하 | 100명 중 95등의 점수. "거의 대부분의 학생"이 이 점수 이상을 받음 |
| **p99** | 전체 요청의 99%가 이 값 이하 | 100명 중 99등의 점수. "단 1명만" 이보다 낮음 |

> 주의: 시험 점수는 높을수록 좋지만, latency는 낮을수록 좋다. 따라서 p99 latency는 "가장 느린 1%의 요청이 이 시간 안에는 완료된다"는 의미이다.

### 구체적 예시

TTFT 측정 결과가 아래와 같다고 하자:

| 백분위수 | TTFT |
|---------|------|
| p50 | 45ms |
| p95 | 120ms |
| p99 | 850ms |

이 경우 해석은 다음과 같다:
- 요청의 절반은 45ms 이내에 첫 토큰을 받는다.
- 요청의 95%는 120ms 이내에 첫 토큰을 받는다.
- 요청의 99%는 850ms 이내에 첫 토큰을 받는다. 나머지 1%는 850ms보다 더 걸린다.

### 프로덕션에서 p99가 중요한 이유

p50만 보면 "평균적으로 빠르니까 괜찮겠지"라고 생각할 수 있다. 하지만 실제 서비스에서는 다음과 같은 상황이 발생한다:

- 하루에 100만 건의 요청이 들어오면, p99 = 850ms라는 것은 **매일 1만 건의 요청이 850ms 이상** 걸린다는 뜻이다.
- 사용자는 "가끔 느린 경험"을 평균보다 더 강하게 기억한다. 10번 중 1번 느리면 "이 서비스 느리다"고 인식한다.
- SLA(서비스 수준 협약)는 보통 p95나 p99 기준으로 설정한다.

따라서 벤치마크 결과를 볼 때 p50뿐 아니라 **p95, p99 값이 급격히 튀지 않는지** 반드시 확인해야 한다.

---

## 4. GPU 메트릭 해석

### 4.1 GPU Memory (VRAM) 사용량

| 항목 | 내용 |
|------|------|
| **정의** | GPU의 비디오 메모리(VRAM) 중 실제 사용 중인 양. 모델 가중치, KV 캐시, 활성화 텐서 등이 VRAM을 차지한다. |
| **단위** | GB 또는 GiB |

**20B 모델의 VRAM 사용량 참고값**:

| 구성 요소 | FP16 (반정밀도) | FP8 | INT4 (GGUF Q4) |
|-----------|----------------|-----|-----------------|
| 모델 가중치 | ~40GB | ~20GB | ~10GB |
| KV 캐시 (가변) | 10~30GB | 10~30GB | 5~15GB |
| 기타 오버헤드 | 2~5GB | 2~5GB | 1~3GB |
| **합계 (추정)** | **52~75GB** | **32~55GB** | **16~28GB** |

> **참고**: A100 80GB에서 FP16 20B 모델을 로드하면 VRAM의 대부분을 사용하게 된다. KV 캐시 크기는 동시 처리 요청 수와 시퀀스 길이에 따라 달라진다.

### 4.2 GPU Utilization (GPU 활용률)

| 항목 | 내용 |
|------|------|
| **정의** | GPU 연산 유닛이 실제로 작업을 수행하고 있는 시간의 비율. `nvidia-smi`에서 확인 가능하다. |
| **단위** | % (0~100) |

**해석 가이드**:

| GPU Utilization | 의미 |
|-----------------|------|
| **90~100%** | GPU가 거의 쉬지 않고 연산 중. 배치 처리나 높은 동시성에서 나타남. 좋은 자원 활용 |
| **50~90%** | 보통 수준. 요청 간 idle 시간이 약간 존재 |
| **< 50%** | GPU가 놀고 있는 시간이 많음. I/O 병목, 낮은 배치 크기, 또는 CPU 바운드 작업이 원인일 수 있음 |
| **낮은 utilization + 높은 VRAM** | 모델은 올라가 있지만 요청이 적거나 처리 대기 중인 상태 |

### 4.3 Peak vs Average Memory

- **Peak Memory**: 벤치마크 전체 구간에서 VRAM 사용량의 최대값. KV 캐시가 가장 많이 찼을 때의 값이다.
- **Average Memory**: 벤치마크 구간의 평균 VRAM 사용량.

Peak memory가 80GB에 가까우면 OOM(Out of Memory) 위험이 있다. 일반적으로 Peak memory가 전체 VRAM의 90% 이내(A100 기준 72GB 이내)에 있는 것이 안전하다.

---

## 5. 양자화 (Quantization) 차이 주의

### 5.1 양자화란?

모델의 가중치(weight)를 원래 정밀도(FP16, 파라미터당 16비트)보다 낮은 비트로 표현하여 메모리 사용량과 연산량을 줄이는 기법이다. 정밀도를 낮추면 속도와 메모리는 개선되지만, 모델 품질(정확도)이 다소 떨어질 수 있다.

### 5.2 주요 양자화 형식 비교

| 형식 | 비트 수 | 파라미터당 메모리 | 사용 프레임워크 | 특징 |
|------|--------|-----------------|----------------|------|
| **FP16** | 16비트 | 2바이트 | vLLM, SGLang | 원본에 가까운 품질. 기본 서빙 형식 |
| **FP8** | 8비트 | 1바이트 | vLLM, SGLang | FP16 대비 메모리 절반, 품질 거의 동일 |
| **GGUF Q8** | 8비트 | ~1바이트 | Ollama | llama.cpp용 양자화 형식 |
| **GGUF Q4_K_M** | 4비트 | ~0.5바이트 | Ollama | 메모리 크게 절약, 품질 일부 저하 |

### 5.3 왜 직접 비교가 어려운가?

이 벤치마크에서 가장 주의할 점은 **Ollama는 GGUF 양자화 모델을 사용하고, vLLM/SGLang은 FP16 모델을 사용한다**는 것이다.

```
vLLM / SGLang:  openai/gpt-oss-20b (FP16, ~40GB)
Ollama:         gpt-oss:20b        (GGUF Q4, ~10GB)
```

이 차이가 벤치마크 결과에 미치는 영향:

| 영향 | 설명 |
|------|------|
| **속도** | Q4는 연산량이 적어 단일 요청에서 토큰 생성이 빠를 수 있음. 하지만 Ollama의 배치 처리 미지원으로 동시 요청에서는 느림 |
| **메모리** | Q4는 VRAM을 ~10GB만 사용하므로, 동일 GPU에서 더 긴 컨텍스트 처리 가능 |
| **품질** | Q4는 FP16 대비 출력 품질이 떨어질 수 있음. 특히 수학, 코딩, 복잡한 추론에서 차이가 날 수 있음 |
| **공정성** | 양자화 방식이 다르므로, throughput이나 latency를 1:1로 비교하는 것은 엄밀히 공정하지 않음 |

> **결론**: Ollama 결과는 "GGUF Q4 양자화 조건에서의 성능"으로 별도 해석해야 하며, vLLM/SGLang의 FP16 결과와 동일 선상에서 비교하면 안 된다.

---

## 6. 프레임워크 비교 시 주의사항

### 6.1 공정한 비교를 위한 조건

프레임워크 간 성능을 공정하게 비교하려면 다음 조건을 맞춰야 한다:

| 조건 | 설명 | 이 프로젝트 적용 |
|------|------|-----------------|
| **동일 모델** | 같은 모델 아키텍처와 가중치 사용 | `openai/gpt-oss-20b` 통일 |
| **동일 정밀도** | FP16이면 모두 FP16으로 | vLLM/SGLang은 FP16, Ollama는 GGUF Q4 (차이 존재) |
| **동일 입력** | 같은 프롬프트, 같은 입출력 길이 | 동일 데이터셋 사용 |
| **동일 하드웨어** | 같은 GPU, 같은 서버 | A100 80GB 동일 서버 |
| **동일 동시성** | 같은 수의 동시 요청 | 벤치마크별 동일 concurrency |

### 6.2 Ollama의 GGUF 제한

Ollama는 llama.cpp 기반이며, GGUF 형식의 양자화 모델만 지원한다. 이로 인해:

- FP16 원본 모델을 직접 로드할 수 없다.
- vLLM/SGLang과 정확히 동일한 모델 가중치로 비교하기 어렵다.
- 연속 배칭(continuous batching) 지원이 제한적이어서 높은 동시성에서 성능이 크게 떨어진다.

### 6.3 Warmup의 중요성

서버를 시작하고 첫 몇 건의 요청은 다음 이유로 느릴 수 있다:

- **CUDA 커널 컴파일**: 첫 실행 시 GPU 커널이 JIT 컴파일됨
- **KV 캐시 할당**: 메모리 풀이 처음 생성됨
- **모델 로딩 지연**: 가중치가 GPU 메모리에 완전히 올라가지 않은 상태

따라서 벤치마크 전에 반드시 **warmup 요청**(보통 3~5건)을 보내고, warmup 구간의 결과는 측정에서 제외해야 한다. warmup 없이 측정하면 TTFT와 Total Latency가 실제보다 높게 나온다.

### 6.4 단일 요청 vs 배치 성능

| 시나리오 | 측정 내용 | 주의점 |
|---------|----------|--------|
| **단일 요청** (concurrency=1) | 프레임워크의 기본 추론 속도 | 배칭 최적화가 적용되지 않으므로 프레임워크 간 차이가 작을 수 있음 |
| **배치/동시 요청** (concurrency=16, 32, 64...) | 프레임워크의 스케줄링/배칭 효율 | vLLM/SGLang의 continuous batching이 큰 차이를 만듦. Ollama는 이 구간에서 불리 |

단일 요청 성능만 보면 "프레임워크 간 차이가 별로 없네?"라고 오해할 수 있다. **반드시 다양한 동시성 수준에서 테스트**해야 프레임워크의 진짜 성능 차이를 볼 수 있다.

### 6.5 서버 설정 차이

각 프레임워크의 기본 설정이 다르므로, 동일 조건을 맞추기 어려운 경우가 있다:

| 설정 항목 | vLLM | SGLang | Ollama |
|-----------|------|--------|--------|
| Max batch size | 설정 가능 | 설정 가능 | 제한적 |
| KV cache 크기 | 자동/수동 설정 | 자동/수동 설정 | 자동 |
| Tensor Parallel | 지원 (--tp) | 지원 (--tp) | 미지원 |
| Prefix caching | 지원 | 기본 활성화 | 미지원 |

벤치마크 결과를 볼 때, 서버 실행 시 사용된 구체적인 옵션(max-model-len, gpu-memory-utilization 등)도 함께 확인해야 한다.

---

## 7. 시나리오별 핵심 지표 요약표

사용 시나리오에 따라 중요한 메트릭이 다르다. 아래 표에서 각 시나리오별로 **가장 중요한 지표**를 정리한다.

| 시나리오 | 가장 중요한 지표 | 중요 | 참고 | 설명 |
|---------|----------------|------|------|------|
| **챗봇 (실시간 대화)** | TTFT, p99 Latency | Token Throughput | Request Throughput | 사용자가 첫 응답을 빠르게 보는 것이 핵심. 스트리밍이므로 TTFT가 체감 속도를 좌우 |
| **배치 처리 (대량 요약/번역)** | Request Throughput, Token Throughput (aggregate) | Total Latency | TTFT | 전체 처리량이 중요. 스트리밍 아니므로 TTFT는 덜 중요 |
| **코드 생성 (Copilot류)** | TTFT, Token Throughput (per-request) | p95 Latency | GPU Memory | 사용자가 타이핑 중 제안을 받으므로 빠른 응답 개시가 핵심 |
| **RAG 파이프라인** | Total Latency, TTFT | Request Throughput | Token Throughput | 긴 컨텍스트(검색 결과 포함)의 prefill 시간과 전체 응답 시간이 중요 |
| **오프라인 분석 (비용 최적화)** | Token Throughput (aggregate), GPU Memory | Request Throughput | TTFT, Latency | GPU 자원 대비 최대 처리량이 핵심. 지연 시간은 덜 중요 |

---

## 8. 일반적 패턴

벤치마크를 수행하면 아래와 같은 패턴이 반복적으로 나타난다. 미리 알아두면 결과를 해석하기 쉽다.

### 8.1 동시 요청에서 SGLang/vLLM이 Ollama보다 훨씬 빠르다

vLLM과 SGLang은 **continuous batching** (연속 배칭)을 지원한다. 여러 요청이 동시에 들어오면 GPU에서 한꺼번에 묶어서 처리하기 때문에, 동시 요청 수가 늘어나도 aggregate throughput이 크게 증가한다.

반면 Ollama(llama.cpp 기반)는 배칭 지원이 제한적이어서 요청을 순차 처리에 가깝게 수행한다. 동시 요청이 10개 이상이면 throughput 차이가 수 배에서 수십 배까지 벌어질 수 있다.

```
예시 (concurrency=32 기준):
  SGLang:  ~2500 tok/s (aggregate)
  vLLM:    ~2200 tok/s (aggregate)
  Ollama:  ~80 tok/s   (aggregate)   <-- 순차 처리에 가까움
```

### 8.2 단일 요청에서는 Ollama도 경쟁력이 있다

동시 요청이 1건일 때는 배칭이 의미가 없으므로, 프레임워크 간 차이가 줄어든다. 특히 Ollama의 GGUF Q4 모델은 가중치 크기가 작아서 메모리 대역폭 병목이 적고, 단일 요청의 토큰 생성 속도가 의외로 빠를 수 있다.

다만 이것은 양자화 효과이지 프레임워크 자체의 우위가 아니라는 점을 기억해야 한다.

### 8.3 SGLang의 Prefix Cache 이점

SGLang은 **RadixAttention 기반의 prefix caching**이 기본 활성화되어 있다. 동일한 시스템 프롬프트나 공통 prefix를 가진 요청이 반복되면, prefill 연산을 캐시에서 재사용하여 TTFT가 크게 줄어든다.

이 이점이 잘 드러나는 경우:
- 동일한 시스템 프롬프트를 사용하는 챗봇 시나리오
- 동일한 문서를 참조하는 RAG 파이프라인
- 반복적인 few-shot 프롬프트

vLLM도 prefix caching을 지원하지만 활성화 여부와 구현 방식이 다를 수 있다.

### 8.4 한국어 토큰화 차이

한국어 텍스트는 영어에 비해 토큰 수가 많아지는 경향이 있다. 같은 의미의 문장이라도 한국어는 영어 대비 1.5~3배 더 많은 토큰으로 쪼개질 수 있다.

이것이 벤치마크에 미치는 영향:
- **Token throughput (tok/s)**: 숫자 자체는 동일해도, 한국어에서는 같은 양의 "의미"를 전달하는 데 더 많은 토큰이 필요하므로 체감 속도가 떨어질 수 있다.
- **TTFT**: 한국어 프롬프트가 더 많은 토큰으로 변환되므로 prefill 시간이 늘어날 수 있다.
- **비용**: 토큰 기반 과금 시 한국어가 더 비쌀 수 있다.

### 8.5 메모리 사용 패턴

일반적으로 관찰되는 메모리 사용 패턴은 다음과 같다:

| 프레임워크 | 메모리 특성 |
|-----------|------------|
| **vLLM** | 시작 시 VRAM을 최대한 미리 할당(pre-allocate)하는 경향. `gpu_memory_utilization` 파라미터로 제어. 초기 메모리 사용량이 높지만 OOM이 적음 |
| **SGLang** | vLLM과 유사하게 KV 캐시를 미리 할당. RadixAttention 캐시가 추가 메모리를 사용할 수 있음 |
| **Ollama** | GGUF Q4 모델 자체가 작아서 VRAM 사용량이 적음. 나머지 VRAM은 유휴 상태로 남음 |

벤치마크 중 `nvidia-smi`로 VRAM 사용량을 모니터링하면, vLLM/SGLang은 거의 80GB 가까이 사용하고 Ollama는 20GB 이하인 것을 확인할 수 있을 것이다. 이 차이는 프레임워크의 비효율이 아니라 **양자화 차이와 메모리 관리 전략의 차이**에서 비롯된다.

---

> 이 가이드는 벤치마크 결과를 읽기 전에 한 번 훑어보면 좋다. 각 숫자가 무엇을 의미하는지, 어떤 숫자를 주의 깊게 봐야 하는지 감을 잡는 데 도움이 될 것이다.
