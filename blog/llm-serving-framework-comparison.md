# LLM 서빙 프레임워크 비교: SGLang vs vLLM vs Ollama

LLM을 서비스에 적용할 때, 모델 선택만큼 중요한 것이 **어떤 서빙 프레임워크를 쓸 것인가**이다. 같은 모델이라도 프레임워크에 따라 처리량이 수십 배 차이 나고, 동시 요청 처리 능력과 운영 복잡도가 크게 달라진다. 오픈소스 LLM 서빙 생태계에서 가장 주목받는 세 프레임워크 -- **SGLang**, **vLLM**, **Ollama** -- 를 동일 조건에서 비교하고, 각각 어떤 상황에 적합한지 정리했다.

---

## 1. 프레임워크 소개와 장단점 비교

### SGLang -- 고성능 구조화 생성 최적화 엔진

SGLang은 UC Berkeley에서 시작된 서빙 프레임워크로, **RadixAttention**이 핵심이다. 기존 PagedAttention이 고정 크기 블록으로 KV 캐시를 관리하는 것과 달리, 기수 트리(Radix Tree) 자료구조를 사용하여 요청 간 접두사를 자동으로 감지하고 공유한다. 이를 통해 챗봇, few-shot 학습 등 공통 시스템 프롬프트를 반복 사용하는 워크로드에서 50~99%의 캐시 적중률을 달성한다.

GPU-CPU 오버랩 스케줄링(Zero-Overhead Scheduler)으로 GPU 유휴 시간을 제거하고, DSL 기반 구조화 생성 최적화도 제공한다. xAI, LinkedIn, Cursor 등에서 프로덕션 운영 중이며, 전 세계 400,000개 이상의 GPU에서 구동되고 있다.

| 장점 | 단점 |
|------|------|
| RadixAttention 기반 자동 접두사 캐싱 (50~99% 적중률) | vLLM 대비 작은 커뮤니티 (Stars 23.5K vs 70.1K) |
| Zero-Overhead 스케줄링으로 GPU 활용률 극대화 | 공식 K8s 배포 참조 구현이 상대적으로 미흡 |
| NVIDIA, AMD, Intel, Google TPU 등 넓은 하드웨어 지원 | GGUF 포맷 지원 제한 |
| 최신 모델 day-0 지원 | 최적 설정을 위한 학습 곡선 존재 |

### vLLM -- 범용 고처리량 추론 엔진

vLLM은 **PagedAttention**으로 유명해진 프레임워크다. 운영체제의 가상 메모리 개념을 KV 캐시 관리에 적용하여, 메모리 낭비를 기존 60~80%에서 4% 미만으로 줄였다. 가장 넓은 모델 호환성(Decoder-only, Encoder-Decoder, MoE, 멀티모달, 임베딩)을 보유하며, HuggingFace, GGUF, SafeTensors 등 모든 주요 포맷을 지원한다.

Amazon Rufus(2.5억 고객), LinkedIn(50+ AI 기능), Meta, Roblox 등 대규모 프로덕션 환경에서 검증되었다. 공식 Production Stack으로 K8s 네이티브 배포를 지원하고, Red Hat의 공식 지원을 받는다.

| 장점 | 단점 |
|------|------|
| 가장 넓은 모델/포맷 호환성 | SGLang 대비 20~29% 낮은 처리량 (독립 벤치마크) |
| 대규모 프로덕션 배포 사례 풍부 | Prefix Caching이 수동적 -- 명시적 설정 필요 |
| PagedAttention으로 KV 캐시 메모리 4% 미만 낭비 | 멀티 노드 서빙에 Ray 의존성 |
| K8s 네이티브 Production Stack 공식 제공 | 소스 빌드 시 의존성 충돌 빈번 |
| 70K+ Stars, 강력한 커뮤니티와 기업 지원 | |

### Ollama -- 로컬 LLM 간편 실행 도구

Ollama는 llama.cpp 위에 구축된 추상화 계층으로, **한 줄 설치**(`curl -fsSL https://ollama.com/install.sh | sh`)와 `ollama pull`, `ollama run`만으로 LLM을 바로 사용할 수 있다. Python이나 CUDA 설치 없이 Go 바이너리 하나로 동작하며, CPU만으로도 추론이 가능하다. GGUF 기반 양자화(기본 Q4_K_M)로 메모리 사용량을 최소화한다.

GitHub Stars 158K로 세 프레임워크 중 가장 높은 인기를 보유하지만, 연속 배칭(Continuous Batching) 미지원, 텐서/파이프라인 병렬 미지원 등 서빙 엔진으로서의 한계가 명확하다.

| 장점 | 단점 |
|------|------|
| 압도적 설치 편의성 (한 줄 설치, 데스크톱 앱) | 연속 배칭 미지원 -- 동시 요청 처리 능력 부족 |
| GPU 없이 CPU 추론 가능 | 텐서/파이프라인 병렬 미지원 |
| 완전한 로컬 프라이버시 | Speculative Decoding, Prefix Caching 없음 |
| HuggingFace 45,000+ GGUF 모델 즉시 사용 | GPTQ, AWQ, FP8 등 GPU 최적화 양자화 미지원 |
| Open WebUI, LangChain 등 풍부한 생태계 통합 | 대규모 프로덕션 배포 사례 부재 |

### 핵심 기술 비교 요약

| 항목 | SGLang | vLLM | Ollama |
|------|--------|------|--------|
| 어텐션 백엔드 | RadixAttention | PagedAttention | llama.cpp 기본 |
| KV 캐시 관리 | 기수 트리 + 자동 접두사 공유 | 블록 기반 + 수동/자동 Prefix Caching | 기본 (고급 관리 없음) |
| 스케줄러 | Zero-Overhead 오버랩 | 연속 배칭 | 순차 처리 |
| 연속 배칭 | O | O | X |
| 텐서 병렬 | O | O | X |
| 구조화 출력 | JSON, 정규식, 문법 | JSON, 정규식, 문법 | JSON |
| Speculative Decoding | O | O | X |
| Multi-LoRA | O | O | X |

---

## 2. 벤치마크 환경과 방법론

### 테스트 환경

| 항목 | 스펙 |
|------|------|
| GPU | NVIDIA A100-SXM4-80GB (80 GB VRAM, NVLink) |
| CPU | Intel Xeon Platinum 8468, 96 Core / 192 Thread |
| RAM | ~1 TiB |
| OS | Ubuntu 22.04.3 LTS |
| CUDA | 12.1 |
| SGLang | v0.5.6.post2 (pip) |
| vLLM | v0.15.1 (pip) |
| Ollama | v0.15.6 (binary) |
| 모델 | openai/gpt-oss-20b (SGLang/vLLM: FP16, Ollama: GGUF Q4) |

### 테스트 시나리오

4가지 시나리오로 프레임워크의 핵심 아키텍처 차이가 실제 성능에 미치는 영향을 검증한다.

| 시나리오 | 목적 | 핵심 변수 |
|---------|------|----------|
| **1. 단일 요청 기본 성능** | 프레임워크 자체의 순수 추론 효율성 | 입력 길이(128/512/1024), 출력 256토큰, 10회 반복 |
| **2. 동시 요청 부하 테스트** | 연속 배칭 vs 순차 처리 동시성 처리 능력 | 동시성(1/8/16/32/64), 100개 요청 |
| **3. 긴 입력 처리 테스트** | Prefill 성능과 메모리 효율성 | 입력 길이(2048/4096), 동시성(1/8) |
| **4. 접두사 캐시 효율성** | RadixAttention vs PagedAttention 캐시 전략 | 공통 시스템 프롬프트(~2048토큰) + 50개 다른 질문 |
| **5. 한국어 성능** | 토크나이저 효율성, 비영어 처리량 차이 | 한국어/영어 대조 프롬프트, 동시성(1/8) |

### 측정 메트릭

| 메트릭 | 설명 |
|--------|------|
| TTFT (Time to First Token) | 요청 전송부터 첫 토큰 수신까지 시간 (ms) |
| Token Throughput | 초당 생성 토큰 수 (tokens/sec) |
| Request Throughput | 초당 완료 요청 수 (requests/sec) |
| Latency (p50/p95/p99) | 요청 완료까지 소요 시간 분포 (ms) |

### 실행 방법

```bash
# 벤치마크 환경 설정 및 실행
bash bench/run_all.sh sglang     # SGLang 벤치마크
bash bench/run_all.sh vllm       # vLLM 벤치마크
bash bench/run_all.sh ollama     # Ollama 벤치마크
```

> **참고**: 모든 프레임워크에서 `temperature=0` (결정론적 출력), 워밍업 3회 후 측정, 요청당 300초 타임아웃을 적용했다. Ollama는 GGUF Q4 양자화를 사용하므로 FP16 기반 SGLang/vLLM과 직접적인 모델 품질 비교는 아니며, 동일 모델의 프레임워크별 서빙 효율성에 초점을 맞추었다.

---

## 3. 벤치마크 결과 분석

### 시나리오 1: 단일 요청 기본 성능

> 입력 512토큰, 출력 256토큰 기준 (단일 요청)

| 메트릭 | SGLang | vLLM | Ollama |
|--------|--------|------|--------|
| TTFT (avg, ms) | ___ | ___ | ___ |
| Token Throughput (tok/s) | ___ | ___ | ___ |
| Total Latency (p99, ms) | ___ | ___ | ___ |
| GPU Memory (MB) | ___ | ___ | ___ |

단일 요청에서는 세 프레임워크 모두 배칭 최적화 없이 순수 추론만 수행하므로, SGLang과 vLLM의 처리량 차이가 크지 않을 것으로 예상된다. Ollama는 GGUF Q4 양자화로 모델 크기가 작아 TTFT에서 경쟁력을 보일 수 있지만, Token Throughput은 llama.cpp 기반 추론의 한계로 낮을 것으로 보인다.

### 시나리오 2: 동시 요청 부하 테스트

> 입력 512토큰, 출력 256토큰, 총 100개 요청

**Token Throughput (tok/s)**

| 동시성 | SGLang | vLLM | Ollama |
|--------|--------|------|--------|
| 1 | ___ | ___ | ___ |
| 8 | ___ | ___ | ___ |
| 16 | ___ | ___ | ___ |
| 32 | ___ | ___ | ___ |
| 64 | ___ | ___ | ___ |

**Latency p99 (ms)**

| 동시성 | SGLang | vLLM | Ollama |
|--------|--------|------|--------|
| 1 | ___ | ___ | ___ |
| 8 | ___ | ___ | ___ |
| 16 | ___ | ___ | ___ |
| 32 | ___ | ___ | ___ |
| 64 | ___ | ___ | ___ |

이 시나리오가 프레임워크 간 가장 큰 차이를 보이는 구간이다. SGLang과 vLLM은 연속 배칭 덕분에 동시성이 증가할수록 처리량이 올라간다. 반면 Ollama는 순차 처리 구조로 인해 동시성이 높아질수록 레이턴시가 선형적으로 증가하고, 64 동시 요청에서는 타임아웃이 발생할 가능성이 높다.

### 시나리오 3: 긴 입력 처리 테스트

> 출력 256토큰 고정

| 설정 | SGLang TTFT (ms) | vLLM TTFT (ms) | Ollama TTFT (ms) |
|------|-----------------|----------------|-----------------|
| 2048토큰, 동시성 1 | ___ | ___ | ___ |
| 2048토큰, 동시성 8 | ___ | ___ | ___ |
| 4096토큰, 동시성 1 | ___ | ___ | ___ |
| 4096토큰, 동시성 8 | ___ | ___ | ___ |

SGLang과 vLLM은 Chunked Prefill을 지원하여 긴 입력에서도 안정적인 TTFT를 유지할 것으로 예상된다. Ollama는 입력 길이 증가에 따라 TTFT가 크게 늘어날 수 있다.

### 시나리오 4: 접두사 캐시 효율성 테스트

> 공통 시스템 프롬프트(~2048토큰) + 50개 다른 사용자 질문, 순차 처리

| 메트릭 | SGLang | vLLM | Ollama |
|--------|--------|------|--------|
| 첫 5개 요청 TTFT (avg, ms) | ___ | ___ | ___ |
| 나머지 요청 TTFT (avg, ms) | ___ | ___ | ___ |
| 캐시 속도 향상 배율 (첫/후속) | ___x | ___x | ___x |
| 전체 처리 시간 (sec) | ___ | ___ | ___ |

이 시나리오는 RadixAttention(SGLang)과 PagedAttention+PrefixCaching(vLLM)의 캐시 전략 차이를 직접 비교한다. SGLang은 자동 접두사 감지로 별도 설정 없이 캐시 효과가 나타나고, vLLM은 `--enable-prefix-caching` 옵션 활성화가 필요하다. Ollama는 캐시 메커니즘이 없어 모든 요청에서 동일한 TTFT를 보일 것이다.

### 시나리오 5: 한국어 성능 테스트

한국어 서비스를 운영할 경우, 토크나이저의 한국어 처리 효율이 실질적인 성능에 큰 영향을 미친다. 동일한 의미의 한국어/영어 프롬프트를 대조 실험하여, 각 프레임워크의 한국어 처리 특성을 확인한다.

#### 한국어 vs 영어 토큰 효율성 (단일 요청)

| 프롬프트 유형 | SGLang 한국어 출력 토큰 | SGLang 영어 출력 토큰 | vLLM 한국어 | vLLM 영어 | Ollama 한국어 | Ollama 영어 |
|-------------|---------------------|--------------------|-----------|---------|-----------| ---------|
| 짧은 질문 | ___ | ___ | ___ | ___ | ___ | ___ |
| 에세이 | ___ | ___ | ___ | ___ | ___ | ___ |
| 기술 설명 | ___ | ___ | ___ | ___ | ___ | ___ |

#### 한국어 처리량 비교 (단일 요청)

| 메트릭 | SGLang (한국어) | SGLang (영어) | vLLM (한국어) | vLLM (영어) | Ollama (한국어) | Ollama (영어) |
|--------|--------------|-------------|-------------|-----------|--------------|-------------|
| TTFT (avg, ms) | ___ | ___ | ___ | ___ | ___ | ___ |
| Token Throughput (tok/s) | ___ | ___ | ___ | ___ | ___ | ___ |
| Total Latency (avg, ms) | ___ | ___ | ___ | ___ | ___ | ___ |

한국어는 영어와 달리 하나의 음절이 여러 토큰으로 분할되는 경우가 많아, 같은 내용을 전달하는 데 더 많은 토큰을 소비한다. 이 차이는 모델의 토크나이저 학습 데이터에 한국어가 얼마나 포함되었는지에 따라 달라진다. Token throughput(tok/s)이 동일하더라도, 한국어는 토큰당 담는 정보량이 적어 **실질적인 정보 처리 속도**는 영어보다 낮을 수 있다.

Ollama는 GGUF 포맷의 토크나이저를 사용하므로, SGLang/vLLM의 HuggingFace 토크나이저와 한국어 토큰화 효율이 다를 수 있다. 특히 GGUF 변환 과정에서 토크나이저가 변경되는 경우, 한국어 토큰 오버헤드가 더 커질 수 있다.

#### 한국어 동시 요청 처리 (동시성 8)

| 프롬프트 유형 | SGLang (tok/s) | vLLM (tok/s) | Ollama (tok/s) |
|-------------|---------------|-------------|---------------|
| 짧은 질문 (한국어) | ___ | ___ | ___ |
| 에세이 (한국어) | ___ | ___ | ___ |
| 기술 설명 (한국어) | ___ | ___ | ___ |

한국어 서비스에서 동시 요청 처리 능력도 중요하다. 연속 배칭 기반의 SGLang/vLLM은 한국어 프롬프트에서도 동시성 확장이 가능하지만, 한국어의 높은 토큰 소비량으로 인해 KV 캐시 메모리 사용이 영어 대비 더 빠르게 증가할 수 있다.

---

## 4. 프레임워크별 적합 시나리오

### 개인 실험 / 로컬 개발

**추천: Ollama**

`ollama pull` 한 줄로 모델을 받고, `ollama run`으로 바로 대화할 수 있는 간편함은 다른 프레임워크가 따라올 수 없다. GPU 없이 CPU만으로도 동작하고, macOS/Windows/Linux 데스크톱 앱을 제공한다. 프롬프트 실험, 프로토타이핑, 학습 용도에 가장 적합하다. Open WebUI와 결합하면 ChatGPT 유사한 웹 인터페이스도 간단히 구축 가능하다.

### 소규모 서빙 (1~10명 내부 사용)

**추천: vLLM**

`vllm serve` 한 줄로 OpenAI 호환 API 서버를 띄울 수 있다. 연속 배칭으로 수명의 동시 사용자를 안정적으로 처리하고, 가장 넓은 모델 호환성으로 다양한 모델을 실험해볼 수 있다. K8s Production Stack이 공식 제공되어 향후 확장도 용이하다.

### 대규모 프로덕션 서빙

**추천: SGLang**

처리량이 최우선인 프로덕션 환경에서는 SGLang이 유리하다. RadixAttention의 자동 접두사 캐싱은 챗봇, 에이전트 등 동일 시스템 프롬프트를 반복하는 워크로드에서 특히 효과적이다. Zero-Overhead 스케줄링으로 GPU 활용률을 극대화하고, Rust 기반 sgl-router로 멀티 노드 분산 서빙도 지원한다. 다만 K8s 배포 도구가 필요하다면 vLLM의 Production Stack도 충분히 좋은 선택이다.

| 사용 시나리오 | 1순위 | 2순위 | 비고 |
|-------------|------|------|------|
| 로컬 실험/프로토타이핑 | Ollama | vLLM | Ollama는 비개발자도 접근 가능 |
| 소규모 내부 서빙 | vLLM | SGLang | 모델 호환성과 생태계 우선 |
| 대규모 프로덕션 (처리량 중심) | SGLang | vLLM | RadixAttention 캐시 효율 우위 |
| 접두사 공유 워크로드 (챗봇, 에이전트) | SGLang | vLLM | 자동 접두사 캐싱 핵심 |
| K8s 기반 엔터프라이즈 배포 | vLLM | SGLang | vLLM Production Stack 공식 제공 |
| 에지/프라이버시 중심 | Ollama | - | 완전한 로컬 추론 |
| 한국어 서비스 | SGLang / vLLM | - | 토크나이저 효율 확인 필수, GGUF 변환 시 한국어 성능 저하 주의 |

---

## 5. RTX 5090 실서빙 환경 권장사항

A100 벤치마크 결과를 RTX 5090 기반 실서빙 환경에 적용할 때 다음 사항을 고려해야 한다.

### VRAM 차이

RTX 5090은 32GB VRAM으로, A100 80GB의 40% 수준이다. 20B 모델을 FP16으로 로드하면 ~40GB가 필요하므로, **반드시 양자화가 필요하다**. FP8(~20GB)이나 INT4(~10GB)를 적용하면 KV 캐시 공간도 확보할 수 있다.

| 양자화 | 모델 크기 (20B 기준) | RTX 5090 적합성 |
|--------|-------------------|----------------|
| FP16 | ~40GB | X (VRAM 초과) |
| FP8 | ~20GB | O (KV 캐시 ~12GB 여유) |
| INT4 | ~10GB | O (KV 캐시 ~22GB 여유) |
| GGUF Q4 (Ollama) | ~10GB | O |

### 아키텍처 차이

RTX 5090은 Blackwell 아키텍처(컨슈머)로 A100(Ampere, 데이터센터)과 다른 특성을 가진다. NVLink 미지원으로 멀티 GPU 시 PCIe 대역폭에 제한되며, ECC 메모리가 없어 장시간 서빙 시 안정성 모니터링이 필요하다. 반면 FP4 연산을 네이티브 지원하므로 INT4/FP4 양자화 성능은 A100보다 나을 수 있다.

### 프레임워크 선택 시 고려

- **Ollama**: RTX 5090에서 가장 간편하게 사용 가능. GGUF Q4는 ~10GB로 VRAM 여유 충분.
- **vLLM**: FP8 양자화 + `--gpu-memory-utilization 0.9` 설정 권장. 단일 GPU에서 안정적.
- **SGLang**: FP8 또는 INT4 양자화 필수. `--mem-fraction-static` 파라미터로 KV 캐시 비율 조정 필요.
- **공통**: NVIDIA Driver 560+ 및 CUDA 12.4+ 설치 권장. Blackwell 아키텍처 지원이 안정화된 프레임워크 버전 사용 필요.

### 소비전력과 냉각

RTX 5090의 TDP는 575W로 A100(400W)보다 높다. 24시간 서빙 시 전력 비용과 냉각 시스템을 점검해야 한다. 서버 랙이 아닌 데스크톱 환경이라면 주변 온도와 케이스 환기에 특히 주의가 필요하다.

---

## 6. 최종 추천과 선택 기준 체크리스트

프레임워크 선택은 단일 기준이 아니라 여러 조건의 조합으로 결정된다. 아래 체크리스트를 순서대로 확인하면 적합한 프레임워크를 빠르게 좁힐 수 있다.

### 의사결정 체크리스트

| # | 질문 | 예 -> 추천 | 아니오 -> 다음 질문 |
|---|------|----------|-------------------|
| 1 | GPU 없이 CPU로만 실행해야 하는가? | **Ollama** | 다음 |
| 2 | 설치 5분 안에 LLM을 사용하고 싶은가? (프로토타이핑) | **Ollama** | 다음 |
| 3 | 동시 사용자 10명 이상을 서빙해야 하는가? | SGLang 또는 vLLM (아래 4~6번) | **Ollama** |
| 4 | K8s 기반 엔터프라이즈 배포가 필요한가? | **vLLM** (Production Stack) | 다음 |
| 5 | 동일 시스템 프롬프트를 반복하는 워크로드인가? (챗봇, 에이전트) | **SGLang** (RadixAttention) | 다음 |
| 6 | 처리량 최대화가 가장 중요한가? | **SGLang** | **vLLM** |

### 프레임워크별 한 줄 요약

- **SGLang**: 처리량과 캐시 효율을 극대화해야 하는 프로덕션 서빙에 최적.
- **vLLM**: 넓은 모델 호환성, 검증된 프로덕션 사례, K8s 지원이 필요한 엔터프라이즈에 최적.
- **Ollama**: GPU 없이도, 개발 환경 없이도 LLM을 바로 쓰고 싶은 개인/소규모 팀에 최적.

세 프레임워크는 경쟁 관계이기보다 **서로 다른 니즈를 해결하는 도구**다. 로컬에서는 Ollama로 빠르게 실험하고, 서비스로 전환할 때 vLLM이나 SGLang으로 넘어가는 것이 가장 현실적인 경로다.
